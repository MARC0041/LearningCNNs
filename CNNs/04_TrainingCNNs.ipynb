{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ]\n",
    ")\n",
    "trainset = torchvision.datasets.CIFAR(root='/date', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "trainloader = torchvision.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "                                    \n",
    "testset = torchvision.datasets.CIFAR(root='/date', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "testloader = torchvision.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                               shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs - Simple\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i,data in enumerate(trainloader, start=0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Predict and compute loss based on CrossEntropy\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Compute the gradients using backprop\n",
    "        loss.backward()\n",
    "        # Update the weights after calculating gradients\n",
    "        optimizer.step()\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "\n",
    "correct, total = 0,0\n",
    "predictions = []\n",
    "net.eval()\n",
    "\n",
    "for i,data in enumerate(testloader, 0):\n",
    "    inputs, labels = data\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions.append(outputs)\n",
    "    total += labels.size(0) # each batch size\n",
    "    correct += (predicted==labels).sum().item()\n",
    "print(\"The testing set accuracy of the network is %d %%\" % (100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs - With Logging\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "for epoch in range(10):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    for i,data in enumerate(trainloader, start=0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Tally the number of correct predictions\n",
    "        y_pred = net(inputs)\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == labels).sum()\n",
    "        trn_corr += batch_corr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print interim results\n",
    "        if i%100 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "    accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "    \n",
    "    # You may optionally also run test batches for each epoch. \n",
    "    # # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(testloader):\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = net(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            \n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenote - How to check how the dimensions for the Linear layers at the end should be. You can use the following trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers\n",
    "conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "# Grab the first MNIST record\n",
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    break\n",
    "x = X_train.view(1,1,28,28)\n",
    "print(\"Shape 1: \", x.shape)\n",
    "# Perform the first convolution/activation\n",
    "x = F.relu(conv1(x))\n",
    "print(\"Shape 2: \", x.shape)\n",
    "# Run the first pooling layer\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print(\"Pool shape: \", x.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
