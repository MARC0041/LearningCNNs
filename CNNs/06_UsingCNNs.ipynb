{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem with overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torch.utils.data.SubsetRandomSampler\n",
    "indices = np.arange(50000)\n",
    "np.random.shuffle(indices)\n",
    "# Dataloaders\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ]\n",
    ")\n",
    "trainset = torchvision.datasets.CIFAR(root='/date', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "trainloader = torchvision.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                                shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:45000]))\n",
    "val_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=False, sampler = torch.utils.data.SubsetRandomSampler(indices[45000:50000]) )\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR(root='/date', train=False,\n",
    "#                                      download=True, transform=transform)\n",
    "# testloader = torchvision.utils.data.DataLoader(trainset, batch_size=128,\n",
    "#                                                shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example....\n",
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:60000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Toolset\n",
    "1. Regularization Techniques - L1,L2 regularization\n",
    "2. Dropout\n",
    "3. Batch-normalization\n",
    "4. Early Stopping\n",
    "5. Transfer Learning\n",
    "6. Finetuning CNNs\n",
    "7. Torchvision module\n",
    "\n",
    "Question: How to choose all hyperparameters?\n",
    "- l2 reg, dropout, optim (adam, grad desc), batch-norm momentum & epsilon, num epochs for early stopping etc.?\n",
    "\n",
    "Answer: Train many networks with different hyperparams and test in the validation set. Use best performing net in the validation set to know the expected accuracy of the network in new data. \n",
    "\n",
    "```\n",
    "# Sets model in train mode\n",
    "model.train()\n",
    "\n",
    "# Sets net in eval mode\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularization\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "Penalises large weights (${w}$) with the ${\\lambda}$ function term: `weight_decay` term is the lambda. \n",
    "\n",
    "$C=-\\frac{1}{n} \\sum_{x j}\\left[y_j \\ln a_j^L+\\left(1-y_j\\right) \\ln \\left(1-a_j^L\\right)\\right]+\\frac{\\lambda}{2 n} \\sum_w w^2$\n",
    "\n",
    "```\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dropouts\n",
    "```\n",
    "self.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(256*6*6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization\n",
    "\n",
    "$\\begin{aligned} & \\text { Input: Values of } x \\text { over a mini-batch: } \\mathcal{B}=\\left\\{x_{1 \\ldots m}\\right\\} \\text {; } \\\\ & \\text { Parameters to be leamed: } \\gamma, \\beta \\\\ & \\text { Output: }\\left\\{y_i=\\mathrm{BN}_{\\gamma, \\beta}\\left(x_i\\right)\\right\\} \\\\ & \n",
    "\\text { // mini-batch mean } & \\mu_{\\mathcal{B}} \\leftarrow \\frac{1}{m} \\sum_{i=1}^m x \\\\ & \n",
    "\\text { // mini-batch variance } & \\sigma_{\\mathcal{B}}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m\\left(x_i-\\mu_{\\mathcal{B}}\\right)^2 \\\\ & \\\\ & \n",
    "\\text { // normalize } & \\widehat{x}_i \\leftarrow \\frac{x_i-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2+\\epsilon}} \\\\ &  \\\\ & \n",
    "\\text { // scale and shift } & y_{\\mathrm{i}} \\leftarrow \\gamma \\widehat{x}_i+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_i\\right) \\\\ &  \\\\ & \\end{aligned}$\n",
    "\n",
    "\n",
    "```\n",
    "self.bn = nn.BatchNorm2d(num_features=64, eps = 1e-05, momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Implement the sequential module for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n",
    "        \n",
    "        # Implement the fully connected layer for classification\n",
    "        self.fc = nn.Linear(in_features=7*7*20, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning\n",
    "\n",
    "**Features in CNNs:**\n",
    "1. First few layers > `Edges`\n",
    "2. Next few conv layers > `Simple geometrical shapes`\n",
    "3. Last layers > `Parts of an object. e.g. Wheel, window`\n",
    "4. Decision layers (fully connected layers)\n",
    "\n",
    "In practice, instead of randomly initialising the net, we would use a pre-trained network to reduce the training time. You do not need large datasets for training now. You can now fine-tune the network with a smaller dataset. \n",
    "\n",
    "\n",
    "**2 ways of Fine-tuning**\n",
    "- Freezing all layers except last few. Only tuning the last fully connected layer. \n",
    "- Fine tune everything\n",
    "\n",
    "*Typically a good idea to freeze most layers if small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Finetuning CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning in pytorch\n",
    "\n",
    "# Instantiate model\n",
    "model = Net()\n",
    "# Load params from old model\n",
    "model.load_state_dict(torch.load('cifar10_net.pth'))\n",
    "# Change the number of out channels\n",
    "model.fc = nn.Linear(4*4*1024, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Torchvision\n",
    "\n",
    "torchvision comes with many pre-trained models such as resnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for training whole model - not freezing the layers:\n",
    "\n",
    "# Create a model using\n",
    "model = Net()\n",
    "\n",
    "# Load the parameters from the old model\n",
    "model.load_state_dict(torch.load('my_net.pth'))\n",
    "\n",
    "# Change the number of out channels\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for freezing the layers\n",
    "# Import the module\n",
    "import torchvision\n",
    "\n",
    "# Download resnet18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers bar the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the number of output units - predicting 7 possible classes\n",
    "model.fc = nn.Linear(512, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
